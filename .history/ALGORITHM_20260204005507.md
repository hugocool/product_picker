# Pair Selection Algorithm - Technical Deep Dive

## Your Questions Answered

### Is the algorithm optimizing to find the winner overall?

**No - it's optimizing to find the complete ranking**, not just the winner.

The algorithm finds the full leaderboard (1st, 2nd, 3rd, ... Nth place) with minimum comparisons. TrueSkill maintains a probability distribution for each item's true skill, so you get:
- Complete rankings (not just top-1)
- Confidence intervals (how certain we are about each ranking)
- Efficient convergence (fewer comparisons than round-robin)

### How is transitivity factored in?

**Fully integrated through Bayesian inference.**

When you compare A vs B:
1. Both A and B's ratings update (winner's μ increases, loser's decreases)
2. Both items' uncertainty (σ) decreases (we learned something)
3. This affects ALL future comparisons involving A or B

Example:
- You compare A vs B → A wins → A's μ goes up, B's μ goes down
- Now when comparing A vs C, the algorithm "knows" A is strong
- And when comparing B vs C, it "knows" B is weaker
- **Transitivity is implicit**: If A>B and B>C, their updated ratings naturally reflect A>C

TrueSkill uses Bayesian inference to propagate information across the entire ranking graph.

### How much randomness is in the sampling process?

**Almost none - the algorithm is deterministic.**

#### Where randomness could appear:
1. **Fallback only**: If the scoring algorithm completely fails (shouldn't happen), it randomly samples a pair
2. **Tie-breaking**: If two pairs have exactly the same score, Python's `max()` picks arbitrarily

#### What's NOT random:
- Pair selection is deterministic given current ratings
- No Monte Carlo sampling
- No random exploration (it's systematic exploration based on uncertainty)

The algorithm is **greedy and deterministic** - it always picks the pair with the highest information score.

## The Information Gain Formula

```python
# For each candidate pair (left, right):
mu_gap = abs(left.mu - right.mu) + 1e-6
info = (left.sigma + right.sigma) / mu_gap
score = info - 0.75 * repeat_count
```

### Breaking it down:

**Numerator: `left.sigma + right.sigma`**
- High σ = high uncertainty = "we don't know where this item ranks"
- Prioritizes uncertain items (exploration)

**Denominator: `abs(left.mu - right.mu)`**
- Small gap = close matchup = "the outcome is uncertain"
- Prioritizes even matches (exploitation)

**Result: Information gain per comparison**
- High when: both items are uncertain AND closely matched
- Low when: items are far apart in skill or well-established

**Penalty: `0.75 * repeat_count`**
- Soft penalty for pairs compared before
- Allows re-matching if they become highly informative later

## Exploration vs Exploitation Balance

| Strategy | What it does | Why it matters |
|----------|-------------|----------------|
| **Exploration** | Prioritize high-σ items | Find hidden gems, reduce uncertainty |
| **Exploitation** | Prioritize close-μ matchups | Resolve close races, refine rankings |
| **Diversity** | Penalize repeats | Gather broad information across all items |
| **Cooldown** | Exclude just-skipped pairs | Prevent user fatigue, allow fresh perspectives |

## Skip/Draw Behavior

**What happens when you skip or draw:**

1. **Match is recorded** in the database (outcome = "S" or "D")
2. **Ratings DON'T update** for skips (but DO update for draws with TrueSkill draw probability)
3. **Immediate exclusion**: That pair won't show up in the very next comparison
4. **Re-entry allowed**: If that pair becomes highly informative later (high σ, close μ), it can be selected again

**Why allow re-matching?**
- After other comparisons, both items' ratings may have changed significantly
- A pair that was confusing before might be clearer now (or vice versa)
- The algorithm adapts to new information

## Example Scenario

You have 100 pendant images:

1. **Early comparisons**: High σ everywhere, algorithm picks somewhat randomly from high-σ pool
2. **Middle stage**: σ decreases, algorithm focuses on close races (items with similar μ)
3. **Late stage**: Most rankings stable, algorithm targets remaining high-σ items and razor-thin μ gaps
4. **Convergence**: When σ is low everywhere and μ gaps are clear, you're done

**Typical comparison count**: ~O(N log N) instead of O(N²) for full round-robin

## Conservative Score = μ - 3σ

The leaderboard sorts by **conservative score**, not raw μ:

```
conservative_score = mu - 3 * sigma
```

This is a **99.7% confidence lower bound** on true skill:
- Items with many comparisons (low σ) → conservative score ≈ μ
- Items with few comparisons (high σ) → conservative score << μ (heavily penalized)

**Why this matters:**
- Prevents "lucky" items from ranking too high
- Forces items to "prove themselves" through multiple comparisons
- Creates more stable, trustworthy rankings

## Summary

| Question | Answer |
|----------|--------|
| Find just the winner? | No - finds complete ranking |
| Transitivity considered? | Yes - Bayesian inference across all items |
| Randomness? | Minimal - deterministic greedy search |
| Goal? | Maximize information gain per comparison |
| Strategy? | Explore uncertain items, exploit close matchups |
