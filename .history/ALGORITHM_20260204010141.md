# Pair Selection Algorithm - Technical Deep Dive

## Your Questions Answered

### Is the algorithm optimizing to find the winner overall?

**No - it's optimizing to find the complete ranking**, not just the winner.

The algorithm finds the full leaderboard (1st, 2nd, 3rd, ... Nth place) with minimum comparisons. TrueSkill maintains a probability distribution for each item's true skill, so you get:
- Complete rankings (not just top-1)
- Confidence intervals (how certain we are about each ranking)
- Efficient convergence (fewer comparisons than round-robin)

### How is transitivity factored in?

**Fully integrated through Bayesian inference.**

When you compare A vs B:
1. Both A and B's ratings update (winner's μ increases, loser's decreases)
2. Both items' uncertainty (σ) decreases (we learned something)
3. This affects ALL future comparisons involving A or B

Example:
- You compare A vs B → A wins → A's μ goes up, B's μ goes down
- Now when comparing A vs C, the algorithm "knows" A is strong
- And when comparing B vs C, it "knows" B is weaker
- **Transitivity is implicit**: If A>B and B>C, their updated ratings naturally reflect A>C

TrueSkill uses Bayesian inference to propagate information across the entire ranking graph.

### How much randomness is in the sampling process?

**Almost none - the algorithm is deterministic.**

#### Where randomness could appear:
1. **Fallback only**: If the scoring algorithm completely fails (shouldn't happen), it randomly samples a pair
2. **Tie-breaking**: If two pairs have exactly the same score, Python's `max()` picks arbitrarily

#### What's NOT random:
- Pair selection is deterministic given current ratings
- No Monte Carlo sampling
- No random exploration (it's systematic exploration based on uncertainty)

The algorithm is **greedy and deterministic** - it always picks the pair with the highest information score.

## The Information Gain Formula

The algorithm has **two phases** that automatically adapt based on average comparisons:

### Early Phase (avg_games < 5): Exploration & Transitivity

```python
# Prioritize broad coverage - see every item multiple times
exploration_bonus = {
    0 games: 100.0,   # Uncompared items get massive boost
    1-2 games: 50.0,  # Rarely seen items get big boost  
    3-4 games: 20.0   # Less-compared items get moderate boost
}
info = (left.sigma + right.sigma) * 10.0 + exploration_bonus
```

**Goal**: Make sure every item is compared multiple times with different opponents to:
- Establish baseline rankings
- Verify transitivity (A>B, B>C implies A>C)
- Prevent exploitation bias (focusing only on top contenders)

### Late Phase (avg_games >= 5): Close Race Resolution

```python
# Prioritize close matchups to refine rankings
mu_gap = abs(left.mu - right.mu) + 1e-6
info = (left.sigma + right.sigma) / mu_gap + exploration_bonus * 0.3
```

**Goal**: Resolve uncertain close races among top contenders:
- Items with similar μ values (hard to distinguish)
- High-σ items that need refinement
- Final ranking polish

### Settings That Govern Behavior

**Candidate Pool Sizes** (in `choose_next_pair`):
```python
left_pool = min(40, total)   # Consider top 40 high-σ items
right_pool = min(120, total) # Mix with up to 120 items
```
- Larger pools = more diverse comparisons
- Smaller pools = focus on top contenders

**Exploration Bonuses**:
```python
0 games: 100.0  # Must see everything at least once
1-2 games: 50.0 # Verify with multiple comparisons
3-4 games: 20.0 # Establish confidence
```
- Higher values = more exploration
- Lower values = more exploitation

**Phase Threshold**:
```python
exploration_phase = avg_games < 5
```
- Lower threshold = switch to exploitation sooner
- Higher threshold = explore longer

**Repeat Penalty**:
```python
score = info - 0.75 * repeat_count
```
- Higher penalty = less re-matching
- Lower penalty = more re-matching when informative

## Exploration vs Exploitation Balance

| Strategy | What it does | Why it matters |
|----------|-------------|----------------|
| **Exploration** | Prioritize high-σ items | Find hidden gems, reduce uncertainty |
| **Exploitation** | Prioritize close-μ matchups | Resolve close races, refine rankings |
| **Diversity** | Penalize repeats | Gather broad information across all items |
| **Cooldown** | Exclude just-skipped pairs | Prevent user fatigue, allow fresh perspectives |

## Skip/Draw Behavior

**What happens when you skip or draw:**

1. **Match is recorded** in the database (outcome = "S" or "D")
2. **Ratings DON'T update** for skips (but DO update for draws with TrueSkill draw probability)
3. **Immediate exclusion**: That pair won't show up in the very next comparison
4. **Re-entry allowed**: If that pair becomes highly informative later (high σ, close μ), it can be selected again

**Why allow re-matching?**
- After other comparisons, both items' ratings may have changed significantly
- A pair that was confusing before might be clearer now (or vice versa)
- The algorithm adapts to new information

## Example Scenario

You have 100 pendant images:

1. **Early comparisons**: High σ everywhere, algorithm picks somewhat randomly from high-σ pool
2. **Middle stage**: σ decreases, algorithm focuses on close races (items with similar μ)
3. **Late stage**: Most rankings stable, algorithm targets remaining high-σ items and razor-thin μ gaps
4. **Convergence**: When σ is low everywhere and μ gaps are clear, you're done

**Typical comparison count**: ~O(N log N) instead of O(N²) for full round-robin

## Conservative Score = μ - 3σ

The leaderboard sorts by **conservative score**, not raw μ:

```
conservative_score = mu - 3 * sigma
```

This is a **99.7% confidence lower bound** on true skill:
- Items with many comparisons (low σ) → conservative score ≈ μ
- Items with few comparisons (high σ) → conservative score << μ (heavily penalized)

**Why this matters:**
- Prevents "lucky" items from ranking too high
- Forces items to "prove themselves" through multiple comparisons
- Creates more stable, trustworthy rankings

## Summary

| Question | Answer |
|----------|--------|
| Find just the winner? | No - finds complete ranking |
| Transitivity considered? | Yes - Bayesian inference across all items |
| Randomness? | Minimal - deterministic greedy search |
| Goal? | Maximize information gain per comparison |
| Strategy? | Explore uncertain items, exploit close matchups |
